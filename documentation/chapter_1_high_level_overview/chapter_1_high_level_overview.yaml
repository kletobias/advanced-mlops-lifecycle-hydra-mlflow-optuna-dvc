base_name: chapter_1_high_level_overview
extension: md
title: Chapter 1 - High-Level Overview
content_chapter:
- section_num: 0
  section_name: Objectives and Motivations
  content_section:
  - "### 1.1 Objectives and Motivations\n\n- **Robust Reproducibility**  \n  The pipeline\
    \ ensures all data transformations are tracked via DVC. Any version of the dataset\
    \ or code can be exactly reproduced, which is essential for stable, verifiable\
    \ ML pipelines in production.\n\n- **Flexibility and Maintainability**  \n  Hydra\
    \ and Omegaconf power hierarchical configurations: you can shift between data\
    \ versions (`v0`, `v1`...) or transformations (`drop_rare_drgs`, `lag_columns`,\
    \ `rolling_columns`) just by changing a single line in a YAML config instead of\
    \ rewriting scripts.\n\n- **Single Source of Truth**  \n  Paths, hyperparameters,\
    \ and experiment details live in config groups rather than scattered across code.\
    \ This keeps the pipeline maintainable—no duplication of parameters.\n\n- **End-to-End\
    \ Logging and Experiment Tracking**  \n  MLflow hooks into each experiment to\
    \ log artifacts (pickle models, permutation importances) and metrics (RMSE, R2),\
    \ making it straightforward to roll back or compare runs."
- section_num: 1
  section_name: Architecture Highlights
  content_section:
  - "### 1.2 Architecture at a Glance\n\n- **Modular Transformations**  \n  Every\
    \ data-manipulation step is a dedicated Python module under `dependencies/transformations/`.\
    \ The pipeline calls them with Hydra overrides, so it’s easy to add or remove\
    \ transformations without altering the central code.\n\n- **Universal Execution\
    \ Script**  \n  A single `universal_step.py` file processes any transformation.\
    \ It reads what to do from Hydra configs, loads input data, runs the specified\
    \ transformation function, and writes output plus metadata. This pattern unifies\
    \ ingestion, cleaning, feature engineering, and modeling steps under the same\
    \ script.\n\n- **DVC Data Lineage**  \n  Each transformation stage is also declared\
    \ in the pipeline (`configs/pipeline/base.yaml`). DVC ensures a reproducible sequence\
    \ of transformations. A new data version always references exactly which code\
    \ and hyperparameters created it.\n\n- **Optuna Hyperparameter Optimization**\
    \  \n  Two lines of config can switch from “random search” to a more advanced\
    \ approach. The pipeline orchestrates parallel trials for model tuning while respecting\
    \ available CPU cores (validated via custom concurrency checks).\n\n- **Metadata-Driven**\
    \  \n  Every output CSV is accompanied by a JSON file capturing row counts, column\
    \ data types, hashing for integrity checks, etc. This supports audits or debugging\
    \ if the data is later found to be inconsistent.\n\n- **Seamless MLflow Integration**\
    \  \n  Each ML training run logs to the same MLflow folder. The best model is\
    \ automatically saved as a .pkl artifact, with optional random forest or permutation-based\
    \ feature importances logged to CSV. Simple queries in MLflow show how hyperparameters\
    \ affected performance historically.\n\n- **Config Grouping and Overrides**  \n\
    \  The code is decoupled from environment specifics: data version, hyperparameters,\
    \ logging levels, etc. By altering Hydra overrides, you can run the same pipeline\
    \ with different storage backends or hyperparameter sets without rewriting any\
    \ logic.\n\n- **Scalable for Large Teams**  \n  Clear separation of concerns (data\
    \ transformation, logging, model training) and advanced version control (DVC +\
    \ Git) means multiple developers can iterate on different transformations or models\
    \ concurrently, each pinned to a distinct data version.\n\nThese design choices\
    \ demonstrate a thorough MLOps-oriented approach—one that ensures both reproducibility\
    \ and agility for rapid experimentation in a real-world setting."
