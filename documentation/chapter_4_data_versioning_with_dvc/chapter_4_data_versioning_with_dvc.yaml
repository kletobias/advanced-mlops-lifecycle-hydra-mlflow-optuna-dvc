base_name: chapter_4_data_versioning_with_dvc
extension: md
title: Chapter 4 - Data Versioning with DVC
content_chapter:
- section_num: 0
  section_name: Why DVC?
  content_section:
  - "### 4.1 Why DVC?\n\n- **Fully Versioned Data Lineage**  \n  Each transformation\
    \ (drop columns, add lag features, etc.) is declared in YAML files under `configs/pipeline/`.\
    \ DVC captures these pipeline stages—linking outputs (like `v10.csv`) to both\
    \ the script and the previous dataset version. Anyone can restore or reproduce\
    \ the exact data state used for a past model run.\n\n- **Parallel Storage Options**\
    \  \n  You can push or pull data to/from multiple remotes: an S3 bucket (for shared\
    \ team access) or a local NAS (for offline or on-prem uses). Switching between\
    \ them only involves adjusting DVC’s remote config.\n\n- **Seamless Syncing**\
    \  \n  Data file changes (`csv`, `json`, even large DB files) are deduplicated\
    \ and stored in an efficient content-addressable way. This keeps repository size\
    \ manageable while ensuring reproducibility.\n\n- **Confidence in Production**\
    \  \n  Because each data artifact is hashed and tracked, you avoid “data drift”—you’ll\
    \ know exactly which rows were used to train any given model, enabling more reliable\
    \ maintenance, auditing, or model retraining."
- section_num: 1
  section_name: Practical Steps
  content_section:
  - "### 4.2 Practical Steps for Data Version Control\n\n1. **Configure DVC Remotes**\
    \  \n   ```sh\n   dvc remote add -d s3_remote s3://mybucket/my_prefix\n   dvc\
    \ remote add nas_remote /path/to/my_nas\n   ```\n   You can switch between them\
    \ or use both. For example, to pull from S3:\n   ```sh\n   dvc pull -r s3_remote\n\
    \   ```\n   Or to push updates to the NAS:\n   ```sh\n   dvc push -r nas_remote\n\
    \   ```\n\n2. **Declare Pipeline Stages**  \n   `configs/pipeline/base.yaml` enumerates\
    \ each stage:\n   ```yaml\n   - name: v10_lag_columns\n     cmd_python: ${cmd_python}\n\
    \     script: ${universal_step_script}\n     overrides: setup.script_base_name=lag_columns\
    \ transformations=lag_columns ...\n     ...\n   ```\n   This means your pipeline\
    \ code references a single source of truth for how data flows from one version\
    \ to the next. DVC looks at this file to know which outputs (e.g., `./data/v10/v10.csv`)\
    \ must be tracked.\n\n3. **Reproduce the Entire Pipeline**  \n   Running:\n  \
    \ ```sh\n   dvc repro\n   ```\n   verifies each stage. If any dependencies changed\
    \ (like `lag_columns.py` or `v9.csv`), DVC rebuilds the affected outputs. If the\
    \ data and code are up-to-date, DVC skips unnecessary steps.\n\n4. **Publishing\
    \ Changes**  \n   After you confirm everything is correct:\n   ```sh\n   git commit\
    \ -am \"feat: add lag_columns stage\"\n   dvc push -r s3_remote\n   ```\n   This\
    \ ensures both code and data are safely versioned. Teammates can then `git pull`\
    \ + `dvc pull` to replicate your work environment locally or on their server.\n\
    \nBy structuring your project around DVC pipelines and remote storage, you gain\
    \ ironclad reproducibility, easy collaboration, and a clear evolutionary path\
    \ for each dataset version—critical elements for any senior-level MLOps workflow. "
