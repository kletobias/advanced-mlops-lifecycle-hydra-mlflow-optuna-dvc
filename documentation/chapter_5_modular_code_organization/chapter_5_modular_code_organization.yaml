base_name: chapter_5_modular_code_organization
extension: md
title: "Chapter 5 - Modular Code Organization"
content:
- section: 0
  name: "“Dependencies” Folder"
  content:
  - |
    ### 5.1 “Dependencies” Folder

    - **Reusability and Maintainability**  
      Each transformation or utility function sits in its own Python module, under folders like `dependencies/transformations/`, `dependencies/cleaning/`, `dependencies/io/`. This ensures no single script grows too large or chaotic.

    - **Encapsulated Logic**  
      Functions like `lag_columns(df)`, `rolling_columns(df)`, or `drop_rare_drgs(df)` each do exactly one thing. All relevant parameters are passed from Hydra configs, so you don’t hard-code them in Python files.

    - **Centralized I/O**  
      Code that handles CSV reading, writing, or metadata logging (like hashing and file-size checks) lives in `dependencies/io/`. Every pipeline stage calls these same functions, guaranteeing consistent logging and error handling across the project.

    - **Logging Utilities**  
      Shared code for logging is in `dependencies/logging_utils/`. For instance, `log_function_call.py` wraps every function so you can see where it starts, where it ends, and if it fails. It centralizes logic for debugging and traceability.

    - **Modeling**  
      Models and hyperparameter search methods (e.g., `rf_optuna_trial.py`, `ridge_optuna_trial.py`) reside in `dependencies/modeling/`. They all follow a similar structure, so you can easily add new algorithms (like XGBoost) without major refactoring.
- section: 1
  name: "“scripts/” Folder"
  content:
  - |
    ### 5.2 “scripts/” Folder

    - **Universal Entry Points**  
      The `universal_step.py` script drives any transformation or modeling step with minimal code duplication. It parses Hydra configs, reads input data, calls your chosen transformation, and writes results. You never copy-paste a script just to tweak a parameter or path.

    - **Orchestration Flows**  
      Scripts like `orchestrate_dvc_flow.py` show how to chain multiple DVC stages in a single run, or how to handle advanced logic like “regenerate dvc.yaml if a stage changed.” This approach keeps high-level orchestration separate from the nitty-gritty transformation functions.

    - **Low Overhead**  
      Because each script references the `dependencies/` folder for actual logic, the scripts themselves remain lean. Changing a single line in a config file can alter the behavior of the pipeline, without rewriting or branching any Python code.

    This code structure ensures each piece is independently testable and easy to locate. New team members can quickly find where transformations happen, how logs are configured, and where model training logic resides, which is key to a senior-level MLOps setup. 
