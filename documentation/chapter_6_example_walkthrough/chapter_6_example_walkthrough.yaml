base_name: chapter_6_example_walkthrough
extension: md
title: Chapter 6 - Example Walkthrough
content_chapter:
- section_num: 0
  section_name: Adding Lag Columns (v10 → v11)
  content_section:
  - "### 6.1 Single Transformation in Practice\n\nThis walkthrough demonstrates the\
    \ move from `v10.csv` to `v11.csv` by adding lag columns.\n\n1. **Relevant Config**\
    \  \n   ```yaml\n   # configs/data_versions/v10.yaml\n   data_version: v10\n \
    \  description: \"Data aggregated and binned. Next step adds lag features.\"\n\
    \   ```\n\n   ```yaml\n   # configs/transformations/lag_columns.yaml\n   columns_to_transform:\n\
    \     - sum_discharges\n     - severity_1_portion\n     # ...\n     - w_mean_cost\n\
    \   groupby_time_based_cols: [facility_id, apr_drg_code, year]\n   lag1_suffix:\
    \ _lag1\n   shift_periods: 1\n   ```\n\n2. **Runtime Command**  \n   ```sh\n \
    \  python scripts/universal_step.py \\\n     setup.script_base_name=lag_columns\
    \ \\\n     transformations=lag_columns \\\n     data_versions.data_version_input=v10\
    \ \\\n     data_versions.data_version_output=v11\n   ```\n   This tells Hydra\
    \ to load `v10.yaml` for input, `v11.yaml` for output, and run the `lag_columns`\
    \ transformation.\n\n3. **Outputs**  \n   - A new CSV: `./data/v11/v11.csv`  \n\
    \     (with columns like `sum_discharges_lag1`, `severity_1_portion_lag1`, etc.)\n\
    \   - A metadata JSON: `v11_metadata.json` (row count, file hash, etc.)\n\n4.\
    \ **Why Lag Columns?**  \n   This step shifts certain numeric columns by one “year”\
    \ within each `[facility_id, apr_drg_code]` group. It’s a common technique for\
    \ time-series or sequential analysis, letting you reference previous-year values\
    \ in the current year’s modeling."
- section_num: 1
  section_name: Tying It All Together
  content_section:
  - "### 6.2 From Config to Final Artifacts\n\n- **Single Command, Complex Pipeline**\
    \  \n  With `dvc repro --force -P`, you can re-run the entire chain from ingestion\
    \ (`v0.csv`) all the way through this lag step (`v11.csv`) if the code or data\
    \ changed. DVC checks each stage in `configs/pipeline/base.yaml` and executes\
    \ it in order.\n\n- **Metadata and Logging**  \n  Each pipeline step calls shared\
    \ logging utilities, so you can find the transformation logs in `logs/runs/<timestamp>/lag_columns.log`.\
    \ Metadata JSON files confirm exactly how many rows and columns each version has\
    \ and record a file hash for data integrity.\n\n- **Experiment Management**  \n\
    \  If you then run `rf_optuna_trial` on `v11.csv`, you can experiment with new\
    \ features (the lag columns) and see if model metrics improve, all tracked by\
    \ MLflow. Switching to a different data version or a different transformation\
    \ pipeline is just another override. This flexible approach highlights how config-driven\
    \ design accelerates experimentation while preserving reproducibility."
