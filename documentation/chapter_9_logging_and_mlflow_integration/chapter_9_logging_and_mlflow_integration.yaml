base_name: chapter_9_logging_and_mlflow_integration
extension: md
title: "Chapter 9 - Logging and MLflow Integration"
content_chapter:
- section_num: 0
  section_name: "Unified Logging Approach"
  content_section:
  - |
    ### 9.1 Unified Logging Approach

    - **Consistent Hydra + Python Logs**  
      The `setup_logging.py` file configures Python’s logging library to log at both DEBUG and INFO levels. Hydra directs each run’s logs into timestamped directories under `logs/runs/<date_time>`, while your pipeline’s core code writes to a “pipeline log” defined by `log_file_path` in your config.

    - **Automatic Logs in Scripts**  
      In `universal_step.py`, every transformation step logs to the console and to the pipeline log. Additional debug-level messages (entering or leaving each function) come from the `@log_function_call` decorator. This level of detail makes it straightforward to diagnose issues or confirm the correct parameters are being used.

    - **Central Control in `configs/logging_utils/base.yaml`**  
      You specify the log format (`%(asctime)s %(levelname)s:%(message)s`) and output paths for both Hydra runs and the pipeline in a single place. This ensures every pipeline step, model training, or data transformation follows the same conventions.

- section_num: 1
  section_name: "Traceability with MLflow"
  content_section:
  - |
    ### 9.2 Traceability with MLflow

    - **Model Artifact Storage**  
      Both `rf_optuna_trial.py` and `ridge_optuna_trial.py` log final models as artifacts in `./mlruns/`. MLflow also stores metrics (like RMSE, R2) and parameters (e.g., random forest hyperparameters). You can examine them locally with `mlflow ui` or push them to a remote S3-based MLflow server.

    - **Permutation Importances**  
      During the final model run, each script calls `calculate_and_log_importances_as_artifact(...)` to compute permutation-based feature importances. These are persisted as CSV artifacts in MLflow, letting you compare the top features across multiple runs or model variants.

    - **Experiment Consistency**  
      Hydra merges your experiment configuration (train/val/test splits, random seeds) with the model parameters. MLflow records these details automatically once you log them in the script. This means you can reconstruct exactly how each experiment was done—even months later—by referencing the logged parameters and your pipeline’s DVC version.

- section_num: 2
  section_name: "Pipeline Log Example"
  content_section:
  - |
    ### 9.3 Pipeline Log Example

    Below is part of the pipeline log showing each stage run sequentially (e.g., `v0_download_and_save_data`, `v0_sanitize_column_names`), including how metadata is calculated and saved. It highlights:

    - **Stage Execution**  
      Each DVC stage prints a “Running stage” message. Hydra overrides (e.g., `setup.script_base_name=download_and_save_data`) appear in the command, indicating which transformation or step is being executed.

    - **File I/O and Metadata**  
      The pipeline logs read/write operations along with file hashes. For example:
      ```
      [2025-03-21 16:38:05,393][dependencies.metadata.calculate_metadata][INFO] - Metadata successfully saved to /Users/tobias/...
      ```
      This ensures you can track each version of your data artifacts.

    - **MLflow Logging**  
      Whenever you run `rf_optuna_trial` or `ridge_optuna_trial`, MLflow logs appear in the same pipeline log. You see trial metrics, best hyperparameters, and the final model logging.

    - **Timestamps and Order**  
      Each log entry is timestamped, so you know exactly when each step starts and finishes. This is crucial for debugging (e.g., if a step took unexpectedly long) or verifying that all transformations happened in the correct sequence.

    By combining Hydra, Python logging, and MLflow experiment tracking, you get a comprehensive picture of what ran, when it ran, how it was configured, which data version was used, and how the model performed—all in one place. This makes the entire system highly auditable and easier to maintain at scale.
