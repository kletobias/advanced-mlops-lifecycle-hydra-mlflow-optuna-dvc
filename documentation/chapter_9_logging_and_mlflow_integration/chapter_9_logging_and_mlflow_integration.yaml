base_name: chapter_9_logging_and_mlflow_integration
extension: md
title: Chapter 9 - Logging and MLflow Integration
content_chapter:
- section_num: 0
  section_name: Unified Logging Approach
  content_section:
  - "### 9.1 Unified Logging Approach\n\n- **Consistent Hydra + Python Logs**  \n\
    \  The `setup_logging.py` file configures Python’s logging library to log at both\
    \ DEBUG and INFO levels. Hydra directs each run’s logs into timestamped directories\
    \ under `logs/runs/<date_time>`, while your pipeline’s core code writes to a “pipeline\
    \ log” defined by `log_file_path` in your config.\n\n- **Automatic Logs in Scripts**\
    \  \n  In `universal_step.py`, every transformation step logs to the console and\
    \ to the pipeline log. Additional debug-level messages (entering or leaving each\
    \ function) come from the `@log_function_call` decorator. This level of detail\
    \ makes it straightforward to diagnose issues or confirm the correct parameters\
    \ are being used.\n\n- **Central Control in `configs/logging_utils/base.yaml`**\
    \  \n  You specify the log format (`%(asctime)s %(levelname)s:%(message)s`) and\
    \ output paths for both Hydra runs and the pipeline in a single place. This ensures\
    \ every pipeline step, model training, or data transformation follows the same\
    \ conventions."
- section_num: 1
  section_name: Traceability with MLflow
  content_section:
  - "### 9.2 Traceability with MLflow\n\n- **Model Artifact Storage**  \n  Both `rf_optuna_trial.py`\
    \ and `ridge_optuna_trial.py` log final models as artifacts in `./mlruns/`. MLflow\
    \ also stores metrics (like RMSE, R2) and parameters (e.g., random forest hyperparameters).\
    \ You can examine them locally with `mlflow ui` or push them to a remote S3-based\
    \ MLflow server.\n\n- **Permutation Importances**  \n  During the final model\
    \ run, each script calls `calculate_and_log_importances_as_artifact(...)` to compute\
    \ permutation-based feature importances. These are persisted as CSV artifacts\
    \ in MLflow, letting you compare the top features across multiple runs or model\
    \ variants.\n\n- **Experiment Consistency**  \n  Hydra merges your experiment\
    \ configuration (train/val/test splits, random seeds) with the model parameters.\
    \ MLflow records these details automatically once you log them in the script.\
    \ This means you can reconstruct exactly how each experiment was done—even months\
    \ later—by referencing the logged parameters and your pipeline’s DVC version."
- section_num: 2
  section_name: Pipeline Log Example
  content_section:
  - "### 9.3 Pipeline Log Example\n\nBelow is part of the pipeline log showing each\
    \ stage run sequentially (e.g., `v0_download_and_save_data`, `v0_sanitize_column_names`),\
    \ including how metadata is calculated and saved. It highlights:\n\n- **Stage\
    \ Execution**  \n  Each DVC stage prints a “Running stage” message. Hydra overrides\
    \ (e.g., `setup.script_base_name=download_and_save_data`) appear in the command,\
    \ indicating which transformation or step is being executed.\n\n- **File I/O and\
    \ Metadata**  \n  The pipeline logs read/write operations along with file hashes.\
    \ For example:\n  ```\n  [2025-03-21 16:38:05,393][dependencies.metadata.calculate_metadata][INFO]\
    \ - Metadata successfully saved to /Users/tobias/...\n  ```\n  This ensures you\
    \ can track each version of your data artifacts.\n\n- **MLflow Logging**  \n \
    \ Whenever you run `rf_optuna_trial` or `ridge_optuna_trial`, MLflow logs appear\
    \ in the same pipeline log. You see trial metrics, best hyperparameters, and the\
    \ final model logging.\n\n- **Timestamps and Order**  \n  Each log entry is timestamped,\
    \ so you know exactly when each step starts and finishes. This is crucial for\
    \ debugging (e.g., if a step took unexpectedly long) or verifying that all transformations\
    \ happened in the correct sequence.\n\nBy combining Hydra, Python logging, and\
    \ MLflow experiment tracking, you get a comprehensive picture of what ran, when\
    \ it ran, how it was configured, which data version was used, and how the model\
    \ performed—all in one place. This makes the entire system highly auditable and\
    \ easier to maintain at scale."
